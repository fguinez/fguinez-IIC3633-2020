# _Comentario:_ A User-Centric Evaluation Framework for Recommender Systems

Este paper propone una forma de evaluar Sistemas Recomendadores _user-centric_, esta evaluación incluye un total de 32 preguntas divididas en 15 constructos. Los investigadores señalan que responder esta batería de preguntas, debería otorgar una visión global de las cualidades del modelo de manera rentable, además de identificar posibles puntos a mejorar.

Destaca el extenso componente cualitativo que contiene esta investigación, dado que esto no es algo tan utilizado en trabajos similares. La combinación entre metodologías cuantitativa y cualitativas es algo importante en toda área de investigación (Onwuegbuzie y Leech, 2005).

En la sección 5.1 "Short Version of ResQue", se afirma que, dado que las preguntas de un mismo constructo están altamente correlacionadas, hacer una sola pregunta por constructo es suficiente para evaluarlo. La forma tangencial en la que se menciona esta posibilidad difiere de la completitud con la que se evaluó la versión completa de _ResQue_, esto pone en duda su realmente esta versión acortada de _ResQue_ es efectiva.

Se podría argumentar que la evaluación de la versión reducida se escapaba del marco de la investigación, sin embargo, si en el mismo texto se dice que "[a]n important goal of this research was to come up with a **fast** but reliable way to evaluate a recommender system", ver más en profundidad la calidad de la versión reducida habría sido muy beneficioso para la investigación. En otras palabras, la evaluación rigurosa de la batería de preguntas reducida es tan importante, que:

1. **En caso de ser una medida efectiva** que permita confiable similar a la batería completa, es probable que la versión reducida hubiera sido una mejor alternativa para ser presentada como la propuesta principal, dado la inherente mayor rapidez por contar con menos preguntas. Así esta opción cumpliría el objetivo incluso mejor que la propuesta principal del paper.

2. **En caso de ser una medida inefectiva** que al ser aplicada no permite evaluar correctamente un sistema recomendador, lo afirmado en la sección 5.1 es erroneo.

Así, en cualquier caso un análisis más detallado de esto habría sido muy fructífero.

Respecto al trabajo futuro mencionado, este se enfoca en la incorporación de un análisis más profundo de las influencias culturales o del dominio de los datos en el comportamiento de los usuarios. Este me resulta interesante por lo mucho que puede relacionarse con la posible presencia de _bias_ en  alguna parte del proceso de análisis de un sistema recomendador, según las afirmaciones de Baeza-Yates (2018) en [Bias on the web](https://doi.org/10.1145/3209581), la batería de preguntas podría contener fácilmente sesgos cognitivos o culturales.

En conclusión, el paper cumple con sus objetivos al proponer una forma de evaluar Sistemas Recomendadores rápida, efectiva y versatil. Sin embargo, no se aprovecha en su totalidad el potencial de la metodología propuesta. 


## Referencias


 Onwuegbuzie, A. y Leech, N. (2005). On Becoming a Pragmatic Researcher: The Importance of Combining Quantitative and Qualitative Research Methodologies. En _International Journal of Social Research Methodology_, 8:5, 375-387. doi: [10.1080/13645570500402447](https://doi.org/10.1080/13645570500402447)

Baeza-Yates, R. (2018). Bias on the web. ECommun. ACM 61, 6 (June 2018), 54–61. doi: [10.1145/3209581](https://doi.org/10.1145/3209581)